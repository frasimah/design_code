#!/usr/bin/env python3
"""
Vandersanden Scraper - Ð¢Ð¾Ñ‡ÐºÐ° Ð²Ñ…Ð¾Ð´Ð°
"""

import asyncio
from functools import wraps
import click
from pathlib import Path
from rich.console import Console
from rich.table import Table

from config.settings import VandersandenConfig, PROCESSED_DATA_DIR
from src.scraper.products import VandersandenProductScraper
from src.processors.cleaner import DataCleaner
from src.processors.exporter import DataExporter


console = Console()


def async_command(f):
    """Ð”ÐµÐºÐ¾Ñ€Ð°Ñ‚Ð¾Ñ€ Ð´Ð»Ñ async click ÐºÐ¾Ð¼Ð°Ð½Ð´"""
    @wraps(f)
    def wrapper(*args, **kwargs):
        return asyncio.run(f(*args, **kwargs))
    return wrapper


@click.group()
@click.version_option(version="0.1.0")
def cli():
    """Vandersanden Scraper - Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚ Ð´Ð»Ñ ÑÐ±Ð¾Ñ€Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð°Ñ…"""
    pass


@cli.command()
@click.option(
    "--category", "-c",
    default="facade_bricks",
    type=click.Choice(["facade_bricks", "pavers", "facade_panels"]),
    help="ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ñ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð¾Ð² Ð´Ð»Ñ ÑÐºÑ€Ð°Ð¿Ð¿Ð¸Ð½Ð³Ð°"
)
@click.option(
    "--output", "-o",
    default="products",
    help="Ð˜Ð¼Ñ Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ñ„Ð°Ð¹Ð»Ð° (Ð±ÐµÐ· Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð¸Ñ)"
)
@click.option(
    "--format", "-f",
    default="json",
    type=click.Choice(["json", "csv", "excel"]),
    help="Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð°"
)
@click.option(
    "--limit", "-l",
    default=None,
    type=int,
    help="ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ñ‚ÑŒ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð¾Ð² (Ð´Ð»Ñ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ)"
)
@async_command
async def scrape(category: str, output: str, format: str, limit: int):
    """Ð¡ÐºÑ€Ð°Ð¿Ð¿Ð¸Ð½Ð³ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð¾Ð² Vandersanden"""
    
    console.print(f"\n[bold blue]ðŸ§± Vandersanden Scraper[/bold blue]")
    console.print(f"Category: [cyan]{category}[/cyan]")
    console.print(f"Output: [cyan]{output}.{format}[/cyan]\n")
    
    async with VandersandenProductScraper(category=category) as scraper:
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÑÑ‹Ð»ÐºÐ¸
        links = await scraper.get_product_links()
        
        if limit:
            links = links[:limit]
            console.print(f"[yellow]Limited to {limit} products[/yellow]\n")
        
        # Ð¡ÐºÑ€Ð°Ð¿Ð¿Ð¸Ð¼ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ñ‹
        products = await scraper.scrape_batch(links)
    
    # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ
    products = DataCleaner.clean_products(products)
    products = DataCleaner.deduplicate(products)
    products = DataCleaner.filter_valid(products)
    
    # Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼
    exporter = DataExporter()
    output_path = exporter.export(products, output, format)
    
    # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ ÑÐ²Ð¾Ð´ÐºÑƒ
    _show_summary(products)
    
    console.print(f"\n[bold green]âœ“ Done![/bold green] Saved to: {output_path}")


@cli.command()
@click.argument("url")
@click.option("--output", "-o", default=None, help="Ð¤Ð°Ð¹Ð» Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ")
@async_command
async def scrape_one(url: str, output: str):
    """Ð¡ÐºÑ€Ð°Ð¿Ð¿Ð¸Ð½Ð³ Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð° Ð¿Ð¾ URL"""
    
    console.print(f"\n[bold blue]ðŸ§± Scraping single product[/bold blue]")
    console.print(f"URL: [cyan]{url}[/cyan]\n")
    
    async with VandersandenProductScraper() as scraper:
        product = await scraper.scrape(url)
    
    if product:
        product = DataCleaner.clean_product(product)
        
        # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ
        console.print(f"[green]Name:[/green] {product.name}")
        console.print(f"[green]Article:[/green] {product.article}")
        console.print(f"[green]Texture:[/green] {product.texture}")
        console.print(f"[green]Formats:[/green] {len(product.formats)}")
        console.print(f"[green]Projects:[/green] {len(product.projects)}")
        
        if output:
            exporter = DataExporter()
            exporter.export([product], output, "json")
    else:
        console.print("[red]Failed to scrape product[/red]")


@cli.command()
@click.argument("url")
@click.option("--output", "-o", default=None, help="Ð¤Ð°Ð¹Ð» Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ")
@click.option("--headless/--no-headless", default=True, help="Ð—Ð°Ð¿ÑƒÑÐº Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€Ð° Ð² Ñ„Ð¾Ð½Ðµ")
@async_command
async def browser_scrape(url: str, output: str, headless: bool):
    """Ð¡ÐºÑ€Ð°Ð¿Ð¿Ð¸Ð½Ð³ Ñ Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€Ð¾Ð¼ (Ð´Ð»Ñ Ñ‚Ð°Ð±Ð¾Ð² ÑˆÐ²Ñ‹/Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ñ‹)"""
    from src.scraper.browser_scraper import BrowserProductScraper
    import json
    
    console.print(f"\n[bold blue]ðŸ§± Browser scraping (with tabs)[/bold blue]")
    console.print(f"URL: [cyan]{url}[/cyan]")
    console.print(f"Headless: [cyan]{headless}[/cyan]\n")
    
    async with BrowserProductScraper(headless=headless) as scraper:
        product_data = await scraper.scrape_product(url)
    
    if product_data:
        # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ
        console.print(f"[green]Name:[/green] {product_data.get('name')}")
        console.print(f"[green]Article:[/green] {product_data.get('article')}")
        console.print(f"[green]Texture:[/green] {product_data.get('texture')}")
        
        # Ð¨Ð²Ñ‹
        joints = product_data.get('joints', [])
        console.print(f"\n[bold yellow]Ð¨Ð²Ñ‹ ({len(joints)}):[/bold yellow]")
        for j in joints:
            console.print(f"  â€¢ {j['name']}")
        
        # Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ñ‹
        formats = product_data.get('available_formats', [])
        console.print(f"\n[bold yellow]Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ñ‹ ({len(formats)}):[/bold yellow]")
        for f in formats:
            dims = f.get('dimensions') or 'N/A'
            weight = f.get('weight_kg') or 'N/A'
            console.print(f"  â€¢ {f['name']}: {dims}, {weight} ÐºÐ³")
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼
        if output:
            output_path = PROCESSED_DATA_DIR / f"{output}.json"
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(product_data, f, ensure_ascii=False, indent=2)
            console.print(f"\n[bold green]âœ“ Saved to:[/bold green] {output_path}")
    else:
        console.print("[red]Failed to scrape product[/red]")


@cli.command()
@click.argument("url")
@click.option("--output", "-o", default=None, help="Ð¤Ð°Ð¹Ð» Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ")
@click.option("--headless/--no-headless", default=True, help="Ð—Ð°Ð¿ÑƒÑÐº Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€Ð° Ð² Ñ„Ð¾Ð½Ðµ")
@async_command
async def gemini_scrape(url: str, output: str, headless: bool):
    """Ð¡ÐºÑ€Ð°Ð¿Ð¿Ð¸Ð½Ð³ Ñ Gemini Vision AI (Ð°Ð½Ð°Ð»Ð¸Ð· ÑÐºÑ€Ð¸Ð½ÑˆÐ¾Ñ‚Ð¾Ð²)"""
    from src.scraper.gemini_scraper import GeminiProductScraper
    import json
    
    console.print(f"\n[bold magenta]ðŸ¤– Gemini Vision AI scraping[/bold magenta]")
    console.print(f"URL: [cyan]{url}[/cyan]")
    console.print(f"Headless: [cyan]{headless}[/cyan]\n")
    
    try:
        async with GeminiProductScraper(headless=headless) as scraper:
            product_data = await scraper.scrape_product(url)
        
        if product_data:
            # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ
            console.print(f"\n[bold green]âœ“ Product scraped successfully[/bold green]")
            console.print(f"[green]Name:[/green] {product_data.get('name')}")
            console.print(f"[green]Article:[/green] {product_data.get('article')}")
            console.print(f"[green]Texture:[/green] {product_data.get('texture')}")
            
            # Ð¨Ð²Ñ‹
            joints = product_data.get('joints', [])
            console.print(f"\n[bold yellow]Ð¨Ð²Ñ‹ ({len(joints)}):[/bold yellow]")
            for j in joints:
                name = j.get('name') if isinstance(j, dict) else j
                console.print(f"  â€¢ {name}")
            
            # Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ñ‹
            formats = product_data.get('available_formats', [])
            console.print(f"\n[bold yellow]Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ñ‹ ({len(formats)}):[/bold yellow]")
            for f in formats:
                if isinstance(f, dict):
                    name = f.get('name', 'N/A')
                    dims = f.get('dimensions') or 'N/A'
                    weight = f.get('weight_kg') or 'N/A'
                    console.print(f"  â€¢ {name}: {dims}, {weight} ÐºÐ³")
                else:
                    console.print(f"  â€¢ {f}")
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼
            if output:
                output_path = PROCESSED_DATA_DIR / f"{output}.json"
                with open(output_path, "w", encoding="utf-8") as f:
                    json.dump(product_data, f, ensure_ascii=False, indent=2)
                console.print(f"\n[bold green]âœ“ Saved to:[/bold green] {output_path}")
        else:
            console.print("[red]Failed to scrape product[/red]")
    except Exception as e:
        console.print(f"[red]Error:[/red] {e}")
        raise


@cli.command()
@click.argument("url")
@click.option("--output", "-o", default=None, help="Ð¤Ð°Ð¹Ð» Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ")
@click.option("--headless/--no-headless", default=True, help="Ð—Ð°Ð¿ÑƒÑÐº Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€Ð° Ð² Ñ„Ð¾Ð½Ðµ")
@click.option("--no-gemini", is_flag=True, help="ÐžÑ‚ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ Gemini Ð¾Ð±Ð¾Ð³Ð°Ñ‰ÐµÐ½Ð¸Ðµ")
@async_command
async def robust_scrape(url: str, output: str, headless: bool, no_gemini: bool):
    """Robust ÑÐºÑ€Ð°Ð¿Ð¿Ð¸Ð½Ð³ (100% Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ñ Ð¿Ñ€Ð¾ÐºÑ€ÑƒÑ‚ÐºÐ¾Ð¹ ÐºÐ°Ñ€ÑƒÑÐµÐ»Ð¸)"""
    from src.scraper.robust_scraper import RobustProductScraper
    import json
    
    console.print(f"\n[bold green]ðŸ”„ Robust scraping (100% extraction)[/bold green]")
    console.print(f"URL: [cyan]{url}[/cyan]")
    console.print(f"Headless: [cyan]{headless}[/cyan]")
    console.print(f"Gemini enrichment: [cyan]{not no_gemini}[/cyan]\n")
    
    try:
        async with RobustProductScraper(headless=headless, use_gemini=not no_gemini) as scraper:
            product_data = await scraper.scrape_product(url)
        
        if product_data:
            console.print(f"\n[bold green]âœ“ Product scraped successfully[/bold green]")
            console.print(f"[green]Name:[/green] {product_data.get('name')}")
            console.print(f"[green]Article:[/green] {product_data.get('article')}")
            console.print(f"[green]Texture:[/green] {product_data.get('texture')}")
            
            # Ð¨Ð²Ñ‹
            joints = product_data.get('joints', [])
            console.print(f"\n[bold yellow]Ð¨Ð²Ñ‹ ({len(joints)}):[/bold yellow]")
            for j in joints:
                name = j.get('name') if isinstance(j, dict) else j
                console.print(f"  â€¢ {name}")
            
            # Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ñ‹
            formats = product_data.get('available_formats', [])
            console.print(f"\n[bold yellow]Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ñ‹ ({len(formats)}):[/bold yellow]")
            for f in formats:
                if isinstance(f, dict):
                    name = f.get('name', 'N/A')
                    dims = f.get('dimensions') or 'N/A'
                    weight = f.get('weight_kg') or 'N/A'
                    console.print(f"  â€¢ {name}: {dims}, {weight} ÐºÐ³")
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼
            if output:
                output_path = PROCESSED_DATA_DIR / f"{output}.json"
                with open(output_path, "w", encoding="utf-8") as f:
                    json.dump(product_data, f, ensure_ascii=False, indent=2)
                console.print(f"\n[bold green]âœ“ Saved to:[/bold green] {output_path}")
        else:
            console.print("[red]Failed to scrape product[/red]")
    except Exception as e:
        console.print(f"[red]Error:[/red] {e}")
        raise


@cli.command()
def categories():
    """ÐŸÐ¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¸"""
    
    table = Table(title="Available Categories")
    table.add_column("Key", style="cyan")
    table.add_column("Slug", style="green")
    table.add_column("URL", style="blue")
    
    for key, slug in VandersandenConfig.CATEGORIES.items():
        url = VandersandenConfig.get_category_url(key)
        table.add_row(key, slug, url)
    
    console.print(table)


def _show_summary(products):
    """ÐŸÐ¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ ÑÐ²Ð¾Ð´ÐºÑƒ Ð¿Ð¾ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð°Ð¼"""
    
    table = Table(title=f"Scraped Products ({len(products)} total)")
    table.add_column("Article", style="cyan")
    table.add_column("Name", style="green")
    table.add_column("Formats", justify="right")
    table.add_column("Color", style="yellow")
    
    for p in products[:10]:  # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ðµ 10
        color = p.color.base_color if p.color else "-"
        table.add_row(
            p.article,
            p.name[:30],
            str(len(p.formats)),
            color
        )
    
    if len(products) > 10:
        table.add_row("...", f"and {len(products) - 10} more", "", "")
    
    console.print(table)


def main():
    """Ð¢Ð¾Ñ‡ÐºÐ° Ð²Ñ…Ð¾Ð´Ð°"""
    cli()


if __name__ == "__main__":
    main()
